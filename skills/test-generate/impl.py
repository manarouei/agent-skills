#!/usr/bin/env python3
"""
test-generate skill implementation.

Generate pytest test suite for node implementation following existing patterns.

SYNC-CELERY SAFE: All file operations are synchronous.
"""

from __future__ import annotations

import json
from pathlib import Path
from string import Template
from typing import TYPE_CHECKING, Any

if TYPE_CHECKING:
    from runtime.executor import ExecutionContext


# Test template following the pattern from SKILL.md
TEST_FILE_TEMPLATE = Template("""#!/usr/bin/env python3
\"\"\"
Tests for ${class_name}Node implementation.

Generated by agent-skills/test-generate
Correlation ID: ${correlation_id}
\"\"\"

import pytest
from unittest.mock import Mock, patch
from nodes.${module_name} import ${class_name}Node
from models import Node, WorkflowModel, NodeExecutionData


@pytest.fixture
def mock_workflow():
    \"\"\"Create mock workflow.\"\"\"
    return Mock(spec=WorkflowModel)


@pytest.fixture
def mock_node_data():
    \"\"\"Create mock node data.\"\"\"
    return Mock(spec=Node)


class Test${class_name}Node:
    \"\"\"Unit tests for ${class_name}Node.\"\"\"
    
    def test_node_initialization(self, mock_workflow, mock_node_data):
        \"\"\"Test that node initializes correctly.\"\"\"
        node = ${class_name}Node(mock_node_data, mock_workflow)
        assert node is not None
        assert node.type == "${node_type}"
    
${operation_tests}
    
    def test_execute_invalid_operation(self, mock_workflow, mock_node_data):
        \"\"\"Test error handling for invalid operation.\"\"\"
        node = ${class_name}Node(mock_node_data, mock_workflow)
        mock_node_data.parameters = {"resource": "unknown", "operation": "unknown"}
        
        with pytest.raises(ValueError, match="Unknown resource/operation"):
            node.execute([NodeExecutionData(json_data={"test": "data"})])
    
    def test_execute_with_continue_on_fail(self, mock_workflow, mock_node_data):
        \"\"\"Test that errors are captured when continue_on_fail is True.\"\"\"
        node = ${class_name}Node(mock_node_data, mock_workflow)
        mock_node_data.continue_on_fail = True
        mock_node_data.parameters = {"resource": "invalid", "operation": "invalid"}
        
        result = node.execute([NodeExecutionData(json_data={"test": "data"})])
        assert len(result) == 1
        assert len(result[0]) > 0
        assert "error" in result[0][0].json_data


# Integration tests (skipped by default)
@pytest.mark.integration
@pytest.mark.skip(reason="Requires live API credentials")
class Test${class_name}NodeIntegration:
    \"\"\"Integration tests for ${class_name}Node.\"\"\"
    
    def test_live_api_call(self):
        \"\"\"Test with live API (requires credentials).\"\"\"
        # TODO: Implement when credentials are available
        pass
""")


OPERATION_TEST_TEMPLATE = Template("""    def test_execute_${operation_name}(self, mock_workflow, mock_node_data):
        \"\"\"Test ${operation_display} operation.\"\"\"
        node = ${class_name}Node(mock_node_data, mock_workflow)
        mock_node_data.parameters = {
            "resource": "${resource}",
            "operation": "${operation}"
        }
        
        # Mock API call
        with patch.object(node, '_api_request', return_value={"success": True}):
            result = node.execute([NodeExecutionData(json_data={"test": "data"})])
            
            assert len(result) == 1
            assert len(result[0]) > 0
    """)


def normalize_to_class_name(name: str) -> str:
    """Convert node name to PascalCase class name."""
    # Remove common prefixes
    name = name.replace("node-", "").replace("Node", "")
    
    # Convert to PascalCase
    parts = name.replace("_", "-").split("-")
    return "".join(p.capitalize() for p in parts if p)


def normalize_to_module_name(name: str) -> str:
    """Convert node name to snake_case module name."""
    # Remove common prefixes
    name = name.replace("node-", "").replace("Node", "")
    
    # Convert to snake_case
    return name.replace("-", "_").lower()


def normalize_operation_name(operation: str) -> str:
    """Convert operation value to snake_case test name."""
    # Handle camelCase
    result = ""
    for i, char in enumerate(operation):
        if char.isupper() and i > 0:
            result += "_"
        result += char.lower()
    return result.replace("-", "_")


def extract_operations(node_schema: dict[str, Any]) -> list[dict[str, Any]]:
    """Extract operation definitions from node schema."""
    operations = []
    parameters = node_schema.get("parameters", [])
    
    for param in parameters:
        if param.get("name") == "operation" and "options" in param:
            for option in param["options"]:
                operations.append({
                    "value": option.get("value", ""),
                    "name": option.get("name", ""),
                    "description": option.get("description", ""),
                    "resource": ""  # Will be filled from schema context
                })
            break
    
    # Try to find resource parameter for context
    for param in parameters:
        if param.get("name") == "resource" and "options" in param:
            # For now, use first resource as default
            if param["options"]:
                resource_value = param["options"][0].get("value", "")
                for op in operations:
                    op["resource"] = resource_value
            break
    
    return operations


def generate_operation_tests(operations: list[dict[str, Any]], class_name: str) -> str:
    """Generate test methods for each operation."""
    if not operations:
        return "    # No operations found in schema\n    pass"
    
    tests = []
    for op in operations[:5]:  # Limit to first 5 operations
        op_name = normalize_operation_name(op["value"])
        test_code = OPERATION_TEST_TEMPLATE.substitute(
            operation_name=op_name,
            operation_display=op.get("name", op["value"]),
            class_name=class_name,
            resource=op.get("resource", "unknown"),
            operation=op["value"],
        )
        tests.append(test_code)
    
    return "\n".join(tests)


def execute_test_generate(ctx: ExecutionContext) -> dict[str, Any]:
    """
    Execute the test-generate skill.
    
    Generates pytest test suite for node implementation.
    
    Args:
        ctx: ExecutionContext with correlation_id, inputs, artifacts_dir
        
    Returns:
        dict with test_files_created and test_count
        
    Raises:
        ValueError: If required inputs are missing or invalid
    """
    # Extract inputs
    correlation_id = ctx.inputs.get("correlation_id", ctx.correlation_id)
    node_schema = ctx.inputs.get("node_schema", {})
    files_modified = ctx.inputs.get("files_modified", [])
    allowlist = ctx.inputs.get("allowlist", {})
    
    # Validate inputs
    if not node_schema:
        raise ValueError("node_schema is required")
    
    if not allowlist:
        raise ValueError("allowlist is required for scope enforcement")
    
    ctx.log("test_generation_started", {
        "correlation_id": correlation_id,
        "schema_name": node_schema.get("name", "unknown"),
    })
    
    # Extract node info
    node_name = node_schema.get("name", "UnknownNode")
    node_type = node_schema.get("type", "unknown")
    class_name = normalize_to_class_name(node_name)
    module_name = normalize_to_module_name(node_name)
    
    # Extract operations
    operations = extract_operations(node_schema)
    
    ctx.log("operations_extracted", {
        "operation_count": len(operations),
        "operations": [op["value"] for op in operations[:10]],  # Log first 10
    })
    
    # Generate operation tests
    operation_tests = generate_operation_tests(operations, class_name)
    
    # Generate test file content
    test_content = TEST_FILE_TEMPLATE.substitute(
        correlation_id=correlation_id,
        class_name=class_name,
        module_name=module_name,
        node_type=node_type,
        operation_tests=operation_tests,
    )
    
    # Write test file to artifacts (not directly to target repo for safety)
    artifacts_test_dir = ctx.artifacts_dir / "generated_tests"
    artifacts_test_dir.mkdir(parents=True, exist_ok=True)
    
    test_file_path = artifacts_test_dir / f"test_{module_name}.py"
    test_file_path.write_text(test_content)
    
    ctx.log("test_file_generated", {
        "file_path": str(test_file_path),
        "size_bytes": len(test_content),
    })
    
    # Create test manifest
    test_manifest = {
        "correlation_id": correlation_id,
        "node_name": node_name,
        "test_files": [
            {
                "path": str(test_file_path),
                "type": "unit",
                "operations_covered": [op["value"] for op in operations],
                "test_count": len(operations) + 3,  # operations + 3 general tests
            }
        ],
        "total_tests": len(operations) + 3,
        "coverage_note": "Generated tests provide basic coverage. Manual review recommended.",
    }
    
    manifest_path = ctx.artifacts_dir / "test_manifest.json"
    manifest_path.write_text(json.dumps(test_manifest, indent=2))
    
    ctx.log("test_generation_completed", {
        "files_created": 1,
        "tests_generated": len(operations) + 3,
    })
    
    return {
        "test_files_created": [str(test_file_path)],
        "test_count": len(operations) + 3,
        "manifest_path": str(manifest_path),
        "note": "Tests generated in artifacts. Review before applying to target repo.",
    }
